# ============================================================================
# Nawal AI Component - Environment Configuration Template
# ============================================================================
# Copy this file to .env and customize for your environment
# Never commit .env with real secrets to version control
# ============================================================================

# ----------------------------------------------------------------------------
# Application Settings
# ----------------------------------------------------------------------------
# Environment: development, staging, production
ENV=development

# Application name (for logging, metrics)
APP_NAME=nawal-ai

# Host and port for API server
HOST=0.0.0.0
PORT=8889

# Number of worker processes (default: CPU count)
WORKERS=4

# Log level: DEBUG, INFO, WARNING, ERROR, CRITICAL
LOG_LEVEL=INFO

# Enable debug mode (DO NOT use in production)
DEBUG=false

# ----------------------------------------------------------------------------
# BelizeChain Blockchain Integration
# ----------------------------------------------------------------------------
# Enable blockchain integration (set to false for standalone development)
BLOCKCHAIN_ENABLED=true

# Blockchain WebSocket endpoint
BLOCKCHAIN_WS_URL=ws://localhost:9944

# Blockchain HTTP endpoint
BLOCKCHAIN_HTTP_URL=http://localhost:9933

# Account seed phrase for blockchain transactions (64-character hex or mnemonic)
# CRITICAL: Use environment-specific secrets, never commit real values
BLOCKCHAIN_ACCOUNT_SEED=//Alice

# Substrate chain type (local, dev, testnet, mainnet)
BLOCKCHAIN_CHAIN_TYPE=dev

# Timeout for blockchain RPC calls (seconds)
BLOCKCHAIN_TIMEOUT=30

# ----------------------------------------------------------------------------
# Kinich Quantum Integration
# ----------------------------------------------------------------------------
# Enable Kinich quantum processing (set to false for classical-only mode)
KINICH_ENABLED=true

# Kinich HTTP API endpoint
KINICH_API_URL=http://localhost:8888

# Kinich gRPC endpoint (for low-latency quantum calls)
KINICH_GRPC_URL=grpc://localhost:50052

# Timeout for Kinich API calls (seconds)
KINICH_TIMEOUT=60

# Fallback to classical processing if Kinich unavailable
KINICH_FALLBACK_TO_CLASSICAL=true

# Quantum dimension (number of qubits for quantum feature encoding)
KINICH_QUANTUM_DIM=8

# Enable quantum result caching
KINICH_ENABLE_CACHING=true

# ----------------------------------------------------------------------------
# Pakit Storage Integration
# ----------------------------------------------------------------------------
# Enable Pakit storage for models and datasets (set to false for local storage)
PAKIT_ENABLED=true

# Pakit HTTP API endpoint
PAKIT_API_URL=http://localhost:8080

# Pakit DAG gateway endpoint
PAKIT_DAG_GATEWAY_URL=http://localhost:8081

# Timeout for Pakit API calls (seconds)
PAKIT_TIMEOUT=120

# Store trained models in Pakit (recommended for production)
PAKIT_STORE_MODELS=true

# Store federated learning aggregates in Pakit
PAKIT_STORE_FL_AGGREGATES=true

# Compression algorithm for Pakit storage: zstd, lz4, brotli, none
PAKIT_COMPRESSION=zstd

# Enable deduplication for model checkpoints
PAKIT_DEDUPLICATION=true

# ----------------------------------------------------------------------------
# Database Configuration
# ----------------------------------------------------------------------------
# PostgreSQL database URL for metadata, FL rounds, user profiles
DATABASE_URL=postgresql://belizechain:dev_password@localhost:5432/belizechain_dev

# Database pool size (connections per worker)
DATABASE_POOL_SIZE=20

# Database connection timeout (seconds)
DATABASE_TIMEOUT=10

# Enable SQLAlchemy query logging (DEBUG only)
DATABASE_ECHO=false

# ----------------------------------------------------------------------------
# Redis Cache Configuration
# ----------------------------------------------------------------------------
# Redis URL for caching, job queue, session management
REDIS_URL=redis://localhost:6379/0

# Redis connection pool size
REDIS_POOL_SIZE=50

# Redis key prefix (useful for multi-tenant setups)
REDIS_KEY_PREFIX=nawal:

# Cache TTL in seconds (default: 1 hour)
REDIS_CACHE_TTL=3600

# ----------------------------------------------------------------------------
# Federated Learning Configuration
# ----------------------------------------------------------------------------
# Number of federated learning rounds
FL_NUM_ROUNDS=100

# Minimum number of clients required to start round
FL_MIN_CLIENTS=2

# Fraction of clients to sample per round (0.0-1.0)
FL_FRACTION_FIT=0.5

# Fraction of clients for evaluation
FL_FRACTION_EVALUATE=0.5

# Minimum fit clients (absolute number)
FL_MIN_FIT_CLIENTS=2

# Minimum evaluation clients
FL_MIN_EVALUATE_CLIENTS=2

# Federated learning strategy: FedAvg, FedProx, FedOpt, QFedAvg
FL_STRATEGY=FedAvg

# Server timeout for client connections (seconds)
FL_SERVER_TIMEOUT=600

# Client timeout for training (seconds)
FL_CLIENT_TIMEOUT=3600

# Enable differential privacy for federated learning
FL_DIFFERENTIAL_PRIVACY=true

# Differential privacy epsilon (privacy budget, lower = more private)
FL_DP_EPSILON=1.0

# Differential privacy delta
FL_DP_DELTA=1e-5

# Differential privacy clipping norm
FL_DP_CLIPPING_NORM=1.0

# ----------------------------------------------------------------------------
# Model Configuration
# ----------------------------------------------------------------------------
# Base model architecture: BelizeChainLLM, transformer, hybrid
MODEL_ARCHITECTURE=BelizeChainLLM

# Model size: small (128M), medium (512M), large (2B)
MODEL_SIZE=medium

# Hidden dimension size
MODEL_HIDDEN_DIM=768

# Number of attention heads
MODEL_NUM_HEADS=12

# Number of transformer layers
MODEL_NUM_LAYERS=12

# Dropout rate
MODEL_DROPOUT=0.1

# Activation function: gelu, relu, swish
MODEL_ACTIVATION=gelu

# Maximum sequence length
MODEL_MAX_SEQ_LENGTH=512

# Vocabulary size
MODEL_VOCAB_SIZE=50000

# ----------------------------------------------------------------------------
# Training Configuration
# ----------------------------------------------------------------------------
# Learning rate
TRAIN_LEARNING_RATE=3e-4

# Batch size per device
TRAIN_BATCH_SIZE=32

# Gradient accumulation steps
TRAIN_GRADIENT_ACCUMULATION_STEPS=4

# Number of training epochs
TRAIN_NUM_EPOCHS=10

# Warmup steps
TRAIN_WARMUP_STEPS=1000

# Weight decay (L2 regularization)
TRAIN_WEIGHT_DECAY=0.01

# Max gradient norm (for clipping)
TRAIN_MAX_GRAD_NORM=1.0

# Mixed precision training: no, fp16, bf16
TRAIN_MIXED_PRECISION=fp16

# Enable gradient checkpointing (saves memory)
TRAIN_GRADIENT_CHECKPOINTING=true

# Optimizer: adamw, sgd, adafactor
TRAIN_OPTIMIZER=adamw

# Learning rate scheduler: linear, cosine, constant
TRAIN_LR_SCHEDULER=cosine

# Save checkpoint every N steps
TRAIN_SAVE_STEPS=1000

# Evaluation strategy: steps, epoch, no
TRAIN_EVALUATION_STRATEGY=steps

# Evaluation steps
TRAIN_EVAL_STEPS=500

# ----------------------------------------------------------------------------
# Genome Evolution Configuration
# ----------------------------------------------------------------------------
# Enable evolutionary architecture search
GENOME_EVOLUTION_ENABLED=true

# Population size for genetic algorithm
GENOME_POPULATION_SIZE=50

# Number of evolution generations
GENOME_NUM_GENERATIONS=20

# Mutation rate (0.0-1.0)
GENOME_MUTATION_RATE=0.1

# Crossover rate (0.0-1.0)
GENOME_CROSSOVER_RATE=0.7

# Tournament selection size
GENOME_TOURNAMENT_SIZE=5

# Elitism: number of top genomes to preserve
GENOME_ELITISM=5

# Fitness metric: accuracy, f1, perplexity
GENOME_FITNESS_METRIC=f1

# ----------------------------------------------------------------------------
# Hybrid Teacher-Student Configuration
# ----------------------------------------------------------------------------
# Enable hybrid teacher-student system
HYBRID_ENABLED=true

# Teacher model: DeepSeek-Coder-V3, GPT-4, Claude-3.5
HYBRID_TEACHER_MODEL=DeepSeek-Coder-V3

# Teacher model API endpoint (if using external API)
HYBRID_TEACHER_API_URL=http://localhost:8000/v1

# Teacher model API key (if required)
HYBRID_TEACHER_API_KEY=

# Distillation temperature
HYBRID_DISTILLATION_TEMPERATURE=2.0

# Alpha for loss weighting (0.0 = hard labels only, 1.0 = teacher only)
HYBRID_DISTILLATION_ALPHA=0.5

# Number of distillation epochs
HYBRID_DISTILLATION_EPOCHS=5

# ----------------------------------------------------------------------------
# Security & Privacy
# ----------------------------------------------------------------------------
# Secret key for session management (generate with: openssl rand -hex 32)
SECRET_KEY=your-secret-key-here-change-in-production

# JWT secret for API authentication
JWT_SECRET=your-jwt-secret-here-change-in-production

# JWT algorithm
JWT_ALGORITHM=HS256

# JWT token expiration (seconds, default: 1 hour)
JWT_EXPIRATION=3600

# Enable API rate limiting
RATE_LIMITING_ENABLED=true

# Rate limit: requests per minute
RATE_LIMIT_PER_MINUTE=100

# Enable CORS
CORS_ENABLED=true

# CORS allowed origins (comma-separated)
CORS_ALLOWED_ORIGINS=http://localhost:3000,http://localhost:3001

# Enable differential privacy for training data
DIFFERENTIAL_PRIVACY_ENABLED=true

# Differential privacy epsilon (lower = more private, but less accurate)
DP_EPSILON=1.0

# Differential privacy delta
DP_DELTA=1e-5

# Differential privacy noise multiplier
DP_NOISE_MULTIPLIER=1.1

# Differential privacy max gradient norm
DP_MAX_GRAD_NORM=1.0

# Enable homomorphic encryption (experimental, high overhead)
HOMOMORPHIC_ENCRYPTION_ENABLED=false

# ----------------------------------------------------------------------------
# Monitoring & Observability
# ----------------------------------------------------------------------------
# Enable Prometheus metrics
METRICS_ENABLED=true

# Metrics port
METRICS_PORT=9090

# Enable TensorBoard logging
TENSORBOARD_ENABLED=true

# TensorBoard log directory
TENSORBOARD_LOG_DIR=./logs/tensorboard

# Enable Weights & Biases logging
WANDB_ENABLED=false

# Weights & Biases API key
WANDB_API_KEY=

# Weights & Biases project name
WANDB_PROJECT=nawal-ai

# Weights & Biases entity (username or team)
WANDB_ENTITY=belizechain

# Enable structured logging (JSON format)
STRUCTURED_LOGGING=true

# Sentry DSN for error tracking (optional)
SENTRY_DSN=

# Sentry environment
SENTRY_ENVIRONMENT=development

# ----------------------------------------------------------------------------
# Storage Paths
# ----------------------------------------------------------------------------
# Data directory for datasets
DATA_DIR=./data

# Model checkpoint directory
MODEL_DIR=./checkpoints

# Cache directory for Hugging Face models
CACHE_DIR=./.cache

# Temporary directory
TEMP_DIR=/tmp/nawal

# ----------------------------------------------------------------------------
# gRPC Configuration
# ----------------------------------------------------------------------------
# Enable gRPC server for federated learning
GRPC_ENABLED=true

# gRPC server port
GRPC_PORT=50051

# gRPC max message size (bytes, default: 100MB)
GRPC_MAX_MESSAGE_SIZE=104857600

# gRPC keepalive time (seconds)
GRPC_KEEPALIVE_TIME=60

# gRPC keepalive timeout (seconds)
GRPC_KEEPALIVE_TIMEOUT=20

# ----------------------------------------------------------------------------
# Development & Testing
# ----------------------------------------------------------------------------
# Enable hot reload (development only)
HOT_RELOAD=true

# Enable code profiling
PROFILING_ENABLED=false

# Profiling output directory
PROFILING_OUTPUT_DIR=./profiling

# Mock external services for testing
MOCK_BLOCKCHAIN=false
MOCK_KINICH=false
MOCK_PAKIT=false

# Test database URL (separate from production)
TEST_DATABASE_URL=postgresql://belizechain:test_password@localhost:5432/belizechain_test

# ----------------------------------------------------------------------------
# GPU Configuration
# ----------------------------------------------------------------------------
# CUDA device IDs (comma-separated, -1 for CPU only)
CUDA_VISIBLE_DEVICES=0

# Enable mixed precision training with NVIDIA Apex
APEX_ENABLED=false

# Enable distributed training
DISTRIBUTED_TRAINING=false

# Distributed backend: nccl, gloo, mpi
DISTRIBUTED_BACKEND=nccl

# Master address for distributed training
MASTER_ADDR=localhost

# Master port for distributed training
MASTER_PORT=29500

# World size (total number of processes)
WORLD_SIZE=1

# Rank (process ID, 0 to WORLD_SIZE-1)
RANK=0

# ----------------------------------------------------------------------------
# Advanced Features
# ----------------------------------------------------------------------------
# Enable model quantization (int8, reduces model size)
QUANTIZATION_ENABLED=false

# Quantization method: dynamic, static, qat
QUANTIZATION_METHOD=dynamic

# Enable knowledge distillation
KNOWLEDGE_DISTILLATION_ENABLED=false

# Enable continual learning
CONTINUAL_LEARNING_ENABLED=false

# Enable meta-learning
META_LEARNING_ENABLED=false

# Enable reinforcement learning from human feedback (RLHF)
RLHF_ENABLED=false

# ============================================================================
# End of Configuration
# ============================================================================
